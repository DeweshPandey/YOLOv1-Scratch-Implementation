{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GFfZ65Ihv8Ql"
      },
      "outputs": [],
      "source": [
        "# Pascal VOC ( Visual Object Class )\n",
        "# 20 Classes outdated for present scenerio\n",
        "# You Only Look Once : Unified Real Time Object Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as FT\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n"
      ],
      "metadata": {
        "id": "tI6yRWcUysQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9314d52c-7b55-4c73-db25-eef28fe5f7d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7cc0595922f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7mzgO5d1JKs",
        "outputId": "c27836ec-08e4-4949-c9e8-e3fcee28286d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d aladdinpersson/pascalvoc-yolo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3S1L37aU6Mq",
        "outputId": "3cf3b0c5-450b-433d-ae5c-cd78e1860343"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/aladdinpersson/pascalvoc-yolo\n",
            "License(s): unknown\n",
            "Downloading pascalvoc-yolo.zip to /content\n",
            "100% 4.29G/4.31G [00:27<00:00, 151MB/s]\n",
            "100% 4.31G/4.31G [00:27<00:00, 168MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/pascalvoc-yolo.zip"
      ],
      "metadata": {
        "id": "0dR2pTmHWdwC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "architecture_config = [\n",
        "    (7, 64, 2, 3), # [ kernel_size, number_of_filters or output_channels, stride, padding]\n",
        "    \"M\", # Maxpool\n",
        "    (3, 192, 1, 1),\n",
        "    \"M\",\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    \"M\",\n",
        "    #List\n",
        "    # [ conv1, conv2, number of times this pattern is repeated ] similar to Inception (bottleneck layer used)\n",
        "    [(1,256, 1, 0), (3, 512, 1, 1), 4],\n",
        "     (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "]"
      ],
      "metadata": {
        "id": "iH8sgfwN96MA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock( nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, **kwargs):\n",
        "    super(CNNBlock, self).__init__() #initialization of parent class\n",
        "\n",
        "    self.conv = nn.Conv2d( in_channels, out_channels, bias = False, **kwargs )\n",
        "    self.batchnorm = nn.BatchNorm2d( out_channels )\n",
        "    self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "neiyt7bvAQBa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YoloV1(nn.Module):\n",
        "  def __init__(self, in_channels = 3 , **kwargs):\n",
        "    super(YoloV1, self).__init__()\n",
        "    self.architecture = architecture_config\n",
        "    self.in_channels = in_channels\n",
        "    self.darknet = self._create_conv_layers(self.architecture)\n",
        "    # Conv layer in YOLO . This Darknet can be replaced with Pre Trained ResNet State of Art for better training and result and resize the input size of image\n",
        "    self.fc = self._create_fc(**kwargs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.darknet(x)\n",
        "    # print(x.shape)\n",
        "    return self.fc(torch.flatten(x, start_dim =1)) # start_dim =1 because we dont want to flatten example/batch_size\n",
        "\n",
        "  def _create_conv_layers(self, architecture):\n",
        "    layers = []\n",
        "    in_channels = self.in_channels\n",
        "\n",
        "    for x in architecture:\n",
        "      if type(x) ==tuple:\n",
        "        layers += [\n",
        "            CNNBlock( in_channels, x[1], kernel_size = x[0], stride = x[2] , padding = x[3])\n",
        "        ]\n",
        "        in_channels = x[1]\n",
        "      elif type(x) ==str:\n",
        "        layers +=[ nn.MaxPool2d( kernel_size =(2,2), stride =(2,2) )]\n",
        "\n",
        "      elif type(x) == list:\n",
        "        conv1 = x[0]\n",
        "        conv2 = x[1]\n",
        "        num_repeats = x[2]\n",
        "\n",
        "        for _ in range(num_repeats):\n",
        "          layers += [\n",
        "              CNNBlock( in_channels, conv1[1], kernel_size = conv1[0], stride = conv1[2], padding = conv1[3])\n",
        "          ]\n",
        "\n",
        "          layers += [\n",
        "              CNNBlock( conv1[1], conv2[1], kernel_size = conv2[0], stride = conv2[2], padding = conv2[3])\n",
        "          ]\n",
        "\n",
        "          in_channels = conv2[1]\n",
        "\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def _create_fc(self, split_size, num_boxes, num_classes):\n",
        "    S, B, C = split_size, num_boxes, num_classes\n",
        "    return nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear( 1024 * S * S, 496),\n",
        "        nn.Dropout(0.0),\n",
        "        nn.LeakyReLU(0.1),\n",
        "        nn.Linear(496, S * S * (C + B * 5)) # (S,S, 30 to  C+B+S)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "nPd0T2DdBp7s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting 2 bounding box\n"
      ],
      "metadata": {
        "id": "98HVattQpuO3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tesst(S =7, B = 2, C = 20):\n",
        "  model = YoloV1(split_size=S, num_boxes=B, num_classes=C)\n",
        "  x= torch.randn((2, 3, 448, 448))\n",
        "  print(model(x).shape)"
      ],
      "metadata": {
        "id": "TDay8dMJPuyk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tesst()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay_K5jr9QXuV",
        "outputId": "b167b367-dfc1-413b-855e-85b046189cdb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 1470])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# intersetion over union\n",
        "# non max suppression\n",
        "# mean average precision\n",
        "# cellboxes to boxes\n",
        "# get_bounding boxes\n",
        "# plot image\n",
        "# save checkpoint\n",
        "# load checkpoint"
      ],
      "metadata": {
        "id": "5l5-SEd0fSLD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union( boxes_preds, boxes_labels, box_format = 'midpoint'):\n",
        "\n",
        "  if box_format == \"midpoint\":\n",
        "    box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "    box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "    box1_x2 = boxes_preds[..., 2:3] + boxes_preds[..., 2:3] / 2\n",
        "    box1_y2 = boxes_preds[..., 3:4] + boxes_preds[..., 3:4] / 2\n",
        "    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "    box2_x2 = boxes_labels[..., 2:3] + boxes_labels[..., 2:3] / 2\n",
        "    box2_y2 = boxes_labels[..., 3:4] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "  if box_format == \"corners\":\n",
        "    box1_x1 = boxes_preds[..., 0:1] # to keep the dimension we use slicing\n",
        "    box1_y1 = boxes_preds[..., 1:2]\n",
        "    box1_x2 = boxes_preds[..., 2:3]\n",
        "    box1_y2 = boxes_preds[..., 3:4]\n",
        "    box2_x1 = boxes_labels[..., 0:1]\n",
        "    box2_y1 = boxes_labels[..., 1:2]\n",
        "    box2_x2 = boxes_labels[..., 2:3]\n",
        "    box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "  x1 = torch.max( box1_x1, box2_x1)\n",
        "  x2 = torch.max( box1_x2, box2_x2)\n",
        "  y1 = torch.min( box1_y1, box2_y1)\n",
        "  y2 = torch.min( box1_y2, box2_y2)\n",
        "\n",
        "  intersection = ( x2 - x1 ).clamp(0) * ( y2 - y1 ).clamp(0) # covering edge case of no intersection\n",
        "\n",
        "  box1_area = abs( (box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "  box2_area = abs( (box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "  return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n"
      ],
      "metadata": {
        "id": "UX6q1qFO2QK6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nms( bboxes, iou_threshold, threshold, box_format = 'corners'):\n",
        "  \"\"\"\n",
        "  Does Non Max Suppression given bboxes\n",
        "\n",
        "  Parameters:\n",
        "    bboxes (list) : list of lists containing all bboxes with each bboxes\n",
        "    specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "    iou_threshold (float) : threshold where predicted bboxes is correct\n",
        "    threshold (float) : threshold to remove predicted bboxes ( independent of IoU )\n",
        "    box_format (str) : \"midpoint\" or \"corners\" used to specify bboxes\n",
        "\n",
        "  Returns :\n",
        "    list : bboxes after performing NMS given a specific IoU threshold\n",
        "  \"\"\"\n",
        "\n",
        "  assert type(bboxes) ==list\n",
        "\n",
        "  bboxes = [ box for box in bboxes if box[1] > threshold]\n",
        "  bboxes = sorted(bboxes, key = lambda x: x[1], reverse= True )\n",
        "  bboxes_after_nms = []\n",
        "\n",
        "  while bboxes:\n",
        "    chosen_box = bboxes.pop(0)\n",
        "    bboxes = [\n",
        "        box for box in bboxes\n",
        "        if box[0] != chosen_box[0]\n",
        "        or intersection_over_union(\n",
        "            torch.tensor(chosen_box[2:]),\n",
        "            torch.tensor(box[2:]),\n",
        "            box_format = box_format\n",
        "        )\n",
        "        < iou_threshold\n",
        "    ]\n",
        "\n",
        "    bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "  return bboxes_after_nms\n",
        "\n"
      ],
      "metadata": {
        "id": "_MoJlCxxPnz9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_average_precision( pred_boxes, true_boxes, iou_threshold = 0.5, box_format = 'corners', num_classes = 20):\n",
        "  \"\"\"\n",
        "  Calculates mean average precision\n",
        "\n",
        "  Parameters:\n",
        "    pred_boxes (list) : list of lists containing all bboxes with each bboxes\n",
        "    specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "    true_boxes (list) : same as pred_boxes except all the correct ones\n",
        "    iou_threshold (float) : threshold where predicted bboxes is correct\n",
        "    box_format (str) : \"midpoint\" or \"corners\" used to specify bboxes\n",
        "    num_classes (int) : number of classes\n",
        "\n",
        "  Returns:\n",
        "    float : mAP value across all classes given a specific IoU threshold\n",
        "  \"\"\"\n",
        "\n",
        "  # list storing all the AP for respective classes\n",
        "  average_precisions = []\n",
        "\n",
        "  # used for numerical stability later on\n",
        "  epsilon = 1e-6\n",
        "\n",
        "  for c in range(num_classes):\n",
        "    detections = []\n",
        "    ground_truths = []\n",
        "\n",
        "    # go through all predictions and targets\n",
        "    # and only the ones that belong to the current class c\n",
        "    for detection in pred_boxes:\n",
        "      if detection[1] == c:\n",
        "        detections.append(detection)\n",
        "\n",
        "    for true_box in true_boxes:\n",
        "      if true_box[1] == c:\n",
        "        ground_truths.append(true_box)\n",
        "\n",
        "    # find the amount of bboxes for each training example\n",
        "    # Counter here finds how many ground truth bboxes we get\n",
        "    # for each training example, so let's say img 0 has 3,\n",
        "    # img 1 has 5 then we will obtain a dictionary with:\n",
        "    # amount_bboxes = {0:3, 1:5}\n",
        "    amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "    # We then go through each key, val in this dictionary\n",
        "    # and convert to the following (w.r.t. same example):\n",
        "    # amount_bboxes = { 0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "    for key, val in amount_bboxes.items():\n",
        "      amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "    # sort by box probabilities which is index 2\n",
        "    detections.sort(key = lambda x : x[2] , reverse = True)\n",
        "    TP = torch.zeros((len(detections)))\n",
        "    FP = torch.zeros((len(detections)))\n",
        "    total_true_bboxes = len(ground_truths)\n",
        "\n",
        "    # if none exists for thsi class then we can safely skip\n",
        "    if total_true_bboxes == 0:\n",
        "      continue\n",
        "\n",
        "    for detection_idx, detection in enumerate(detections):\n",
        "      ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
        "\n",
        "      num_gts = len(ground_truth_img)\n",
        "      best_iou = 0\n",
        "\n",
        "      for idx, gt in enumerate(ground_truth_img):\n",
        "        iou = intersection_over_union(\n",
        "            torch.tensor(detection[3:]),\n",
        "            torch.tensor(gt[3:]),\n",
        "            box_format = box_format,\n",
        "        )\n",
        "\n",
        "        if iou > best_iou:\n",
        "          best_iou = iou\n",
        "          best_gt_idx = idx\n",
        "\n",
        "      if best_iou > iou:\n",
        "        # only detect ground truth detection once\n",
        "        if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "          # true positive and add this bounding box to seen\n",
        "          TP[detection_idx] = 1\n",
        "          amount_bboxes[detection[0]][best_gt_idx] =1\n",
        "\n",
        "        else:\n",
        "          FP[detection_idx] = 1\n",
        "\n",
        "      # if IOU is lower then the detection is a false positive\n",
        "      else:\n",
        "        FP[detection_idx] = 1\n",
        "\n",
        "    TP_cumsum = torch.cumsum(TP, dim = 0)\n",
        "    FP_cumsum = torch.cumsum(FP, dim = 0)\n",
        "    recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "    precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "    precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "    recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "    # torch.trapz for numerical integrationq\n",
        "    average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "  return sum(average_precisions) / len(average_precisions)\n"
      ],
      "metadata": {
        "id": "oDXr8mutSiUY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TotalLoss( nn.Module):\n",
        "\n",
        "  def __init__(self, S = 7, B = 2, C = 20):\n",
        "    super(TotalLoss, self).__init__()\n",
        "\n",
        "    self.mse = nn.MSELoss(reduction = \"sum\")\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.lambda_no_obj = 0.5\n",
        "    self.lambda_coord = 5\n",
        "\n",
        "  def forward( self, predictions, target):\n",
        "    print(\"Dewesh\")\n",
        "    print(predictions.shape)\n",
        "    predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
        "    print(predictions.shape)\n",
        "    iou_b1 = intersection_over_union( predictions[..., 21:25], target[..., 21:25])\n",
        "    iou_b2 = intersection_over_union( predictions[..., 26:30], target[..., 21:25])\n",
        "    ious = torch.cat( [iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim =0)\n",
        "    iou_maxes, best_box = torch.max(ious, dim = 0)\n",
        "    exists_box = target[..., 20].unsqueeze(3) # identity of obj_i\n",
        "\n",
        "    # For Box Coordinates\n",
        "    box_predictions = exists_box * (\n",
        "      (  best_box * predictions[..., 21:25] + (1 - best_box) * predictions[..., 26:30] )\n",
        "    )\n",
        "    box_targets = exists_box * target[..., 21:25]\n",
        "\n",
        "    box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
        "        torch.abs(box_predictions[..., 2:4] + 1e-6)\n",
        "    )\n",
        "\n",
        "    box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "    box_loss = self.mse(\n",
        "        torch.flatten(box_predictions, end_dim = -2),\n",
        "        torch.flatten(box_targets, end_dim =-2)\n",
        "    )\n",
        "\n",
        "\n",
        "    # For Object Loss\n",
        "    pred_box = (\n",
        "        best_box * predictions[..., 25:26] + ( 1- best_box) * predictions[..., 20:21]\n",
        "    )\n",
        "\n",
        "    object_loss = self.mse(\n",
        "        torch.flatten(exists_box * pred_box),\n",
        "        torch.flatten(exists_box * target[..., 20:21])\n",
        "    )\n",
        "\n",
        "    # For no object loss\n",
        "\n",
        "    no_obj_loss = self.mse(\n",
        "        torch.flatten( ( 1 - exists_box) * predictions[..., 20:21], start_dim = 1),\n",
        "        torch.flatten( ( 1- exists_box ) * target[..., 20:21], start_dim = 1)\n",
        "    )\n",
        "\n",
        "    no_obj_loss += self.mse(\n",
        "        torch.flatten( ( 1 - exists_box) * predictions[..., 25:26], start_dim = 1),\n",
        "        torch.flatten( ( 1- exists_box ) * target[..., 20:21], start_dim = 1)\n",
        "    )\n",
        "\n",
        "    # class Loss\n",
        "    class_loss = self.mse(\n",
        "        torch.flatten(exists_box * predictions[..., :20], end_dim = -2),\n",
        "        torch.flatten(exists_box * target[..., :20], end_dim = -2)\n",
        "\n",
        "    )\n",
        "\n",
        "    # total loss\n",
        "    loss = (\n",
        "        self.lambda_coord * box_loss\n",
        "        + object_loss\n",
        "        + self.lambda_no_obj * no_obj_loss\n",
        "        + class_loss\n",
        "    )\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "NQvWBbmRQaa2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_image(image, boxes):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "\n",
        "    # box[0] is x midpoint, box[2] is width\n",
        "    # box[1] is y midpoint, box[3] is height\n",
        "\n",
        "    # Create a Rectangle potch\n",
        "    for box in boxes:\n",
        "        box = box[2:]\n",
        "        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=1,\n",
        "            edgecolor=\"r\",\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cpu\",\n",
        "):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = nms(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "\n",
        "            #if batch_idx == 0 and idx == 0:\n",
        "            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n",
        "            #    print(nms_boxes)\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                # many will get converted to 0 pred\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "\n",
        "def convert_cellboxes(predictions, S=7):\n",
        "    \"\"\"\n",
        "    Converts bounding boxes output from Yolo with\n",
        "    an image split size of S into entire image ratios\n",
        "    rather than relative to cell ratios. Tried to do this\n",
        "    vectorized, but this resulted in quite difficult to read\n",
        "    code... Use as a black box? Or implement a more intuitive,\n",
        "    using 2 for loops iterating range(S) and convert them one\n",
        "    by one, resulting in a slower but more readable implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = predictions.to(\"cpu\")\n",
        "    batch_size = predictions.shape[0]\n",
        "    predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
        "    bboxes1 = predictions[..., 21:25]\n",
        "    bboxes2 = predictions[..., 26:30]\n",
        "    scores = torch.cat(\n",
        "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
        "    )\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
        "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "    w_y = 1 / S * best_boxes[..., 2:4]\n",
        "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
        "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n",
        "        -1\n",
        "    )\n",
        "    converted_preds = torch.cat(\n",
        "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
        "    )\n",
        "\n",
        "    return converted_preds\n",
        "\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
      ],
      "metadata": {
        "id": "zSXqje0EbjRt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/input_dataset/kaggle.json ~/.kaggle/n\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "6-zhesmEUdvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a20fd2-9dfb-4259-f814-871b47e75e0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import kagglehub\n",
        "\n",
        "# # Download latest version\n",
        "# path = kagglehub.dataset_download(\"aladdinpersson/pascalvoc-yolo\")\n",
        "\n",
        "# print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "eqvptBMkWU3s"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "  def __init__(\n",
        "      self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform=None\n",
        "  ):\n",
        "    self.annotations = pd.read_csv(csv_file)\n",
        "    self.img_dir = img_dir\n",
        "    self.label_dir = label_dir\n",
        "    self.transform = transform\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "    boxes = []\n",
        "\n",
        "    with open(label_path) as f:\n",
        "\n",
        "      for label in f.readlines():\n",
        "        class_label, x, y, width, height = [ float(x) if float(x) != int(float(x)) else int(x) for x in label.replace(\"\\n\", \"\").split() ]\n",
        "        boxes.append([class_label, x , y, width, height])\n",
        "\n",
        "    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "    image = Image.open(img_path)\n",
        "    boxes = torch.tensor(boxes)\n",
        "\n",
        "    if self.transform:\n",
        "      # image\n",
        "      image, boxes = self.transform( image, boxes)\n",
        "\n",
        "    label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "\n",
        "    for box in boxes:\n",
        "      class_label, x , y, width, height = box.tolist()\n",
        "      class_label = int(class_label)\n",
        "      i ,j = int(self.S*y), int(self.S*x)\n",
        "      x_cell, y_cell  = self.S*x-j, self.S*y-i\n",
        "      width_cell, height_cell = (width*self.S, height*self.S)\n",
        "\n",
        "      if label_matrix[i, j, 20] == 0:\n",
        "        label_matrix[i, j, 20] =1\n",
        "        box_coordinates = torch.tensor(\n",
        "            [x_cell, y_cell, width_cell, height_cell]\n",
        "        )\n",
        "        label_matrix[i, j, 21:25] = box_coordinates\n",
        "        label_matrix[i, j, class_label] = 1\n",
        "\n",
        "    return image, label_matrix\n"
      ],
      "metadata": {
        "id": "WTrKkbngZABm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "C3CEhlcPeSp0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "LEARNING_RATE = 2e-5\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE =16\n",
        "WEIGHT_DECAY = 0\n",
        "# computational reasons\n",
        "EPOCHS = 100\n",
        "NUM_WORKERS = 2\n",
        "PIN_MEMORY = True\n",
        "LOAD_MODEL = False\n",
        "LOAD_MODEL_FILE = \"overfit.pth.tar\"\n",
        "IMG_DIR= \"/content/images\"\n",
        "LABEL_DIR = \"/content/labels\"\n",
        "\n",
        "class Compose(object):\n",
        "  def __init__(self, transforms):\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __call__(self, img, bboxes):\n",
        "    for t in self.transforms:\n",
        "      img, bboxes = t(img), bboxes\n",
        "    return img, bboxes\n",
        "\n",
        "transform = Compose( [transforms.Resize((448, 448)), transforms.ToTensor()])\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "  loop = tqdm(train_loader, leave = True)\n",
        "  mean_loss = []\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(loop):\n",
        "    x, y  = x.to(DEVICE), y.to(DEVICE)\n",
        "    out = model(x)\n",
        "    loss = loss_fn(out, y)\n",
        "    mean_loss.append(loss.item())\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # update progress bar\n",
        "    loop.set_postfix(loss = loss.item())\n",
        "\n",
        "  print(f\"Mean loss was {sum(mean_loss)/len(mean_loss)}\")\n",
        "\n",
        "def main():\n",
        "  model = YoloV1(split_size = 7, num_boxes = 2, num_classes = 20).to(DEVICE)\n",
        "  optimizer = optim.Adam(\n",
        "      model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY\n",
        "  )\n",
        "  loss_fn = TotalLoss()\n",
        "\n",
        "  if LOAD_MODEL:\n",
        "    load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)\n",
        "\n",
        "  train_dataset = VOCDataset(\n",
        "      \"/content/8examples.csv\",\n",
        "      transform = transform,\n",
        "      img_dir = IMG_DIR,\n",
        "      label_dir = LABEL_DIR\n",
        "  )\n",
        "\n",
        "  test_dataset = VOCDataset(\n",
        "      \"/content/test.csv\",\n",
        "      transform = transform,\n",
        "      img_dir = IMG_DIR,\n",
        "      label_dir = LABEL_DIR\n",
        "  )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      dataset = train_dataset,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      num_workers = NUM_WORKERS,\n",
        "      pin_memory = PIN_MEMORY,\n",
        "      shuffle = True,\n",
        "      drop_last = False\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      dataset = test_dataset,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      num_workers = NUM_WORKERS,\n",
        "      pin_memory = PIN_MEMORY,\n",
        "      shuffle = True,\n",
        "      drop_last = True\n",
        "  )\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "\n",
        "\n",
        "    pred_boxes, target_boxes = get_bboxes(\n",
        "        train_loader, model, iou_threshold = 0.5, threshold = 0.4\n",
        "        )\n",
        "\n",
        "    mean_avg_prec = mean_average_precision(\n",
        "        pred_boxes, target_boxes, iou_threshold = 0.5, box_format = \"midpoint\"\n",
        "    )\n",
        "    print(f\"Train mAP: {mean_avg_prec}\")\n",
        "\n",
        "    train_fn( train_loader, model, optimizer, loss_fn)\n",
        "\n",
        "    if mean_avg_prec >0.9 :\n",
        "      checkpoint = {\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict()\n",
        "      }\n",
        "      save_checkpoint(checkpoint, filename = LOAD_MODEL_FILE)\n",
        "      import time\n",
        "      time.sleep(10)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lVEGg5ELfxwh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "P4qP5uLmkVVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18df751b-eef4-44c5-90b1-ec1173b69f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train mAP: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dewesh\n",
            "torch.Size([8, 1470])\n",
            "torch.Size([8, 7, 7, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsKPkJChxzh2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}